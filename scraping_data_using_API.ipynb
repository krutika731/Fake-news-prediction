{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Capstone Project- Fake News Identification\n",
    "\n",
    "### Web scraping  news articles\n",
    "Here, I am using Webhoseio API to scrape news articles available on https://mediabiasfactcheck.com. Here, I am scraping news articles from different sites having factual reporting type High, Mixed and Low. The ***mediabiasfactcheck.com*** does this work manually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import requests\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below code is scraping news articles from diffrent websites. Here the websites are given as string in query parameter.\n",
    "This code is used to scrape the news articles from past 30 days(from the day of crawled)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [],
   "source": [
    "import webhoseio\n",
    "\n",
    "#define API key\n",
    "webhoseio.config(token=\"d4feac70-4b0a-4896-948c-96885d5a962e\")\n",
    "\n",
    "#defining query parameters.\n",
    "#q: list of sites to scrape\n",
    "#ts: time stamp of last 30 days(defined by webhoseio API)\n",
    "#sort: it returns the news articles sorted by the order they are crawled\n",
    "query_params = {\n",
    "    \"q\": \"site:ageofautism.com OR site:10news.one OR site:100percentfedup.com OR site:allnewspipeline.com OR site:americanthinker.com OR site:bannedinformation.com OR site:breaking911.com OR site:eaglerising.com OR site:southafricatoday.net OR site:subjectpolitics.com OR site:ecowatch.com OR site:ancient-code.com OR site:cnn.com OR site:thenewcivilrightsmovement.com OR site:splinternews.com OR site:buzzfeed.com site_category:news OR site:politicaldig.com OR site:ilovemyfreedom.org OR site:trueviralnews.com OR site:altright.com OR site:censored.news OR site:canadafreepress.com OR site:syrianews.cc OR site:bustedlocals.com OR site:bostonreview.net OR site:rabble.ca OR site:healthline.com OR site:abc.net.au OR site: japantimes.co.jp OR site:cnbc.com OR site:livescience.com\",\n",
    "    \"ts\": \"1554956406114\",\n",
    "    \"sort\": \"crawled\"\n",
    "    }\n",
    "\n",
    "# return the JSON of all scraped news article\n",
    "output = webhoseio.query(\"filterWebContent\", query_params)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Preparing the data return by JSON object to to put it in CSV and dataframe for later use\n",
    "Here the return Json object can onlu hold 100 news article(posts) at a time so to get all the data we need to call webhoseio.get_next() method and iterate it untill all news articles are processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_list = []\n",
    "while len(output['posts'])>0:\n",
    "    for index,each_post in enumerate(output['posts']):\n",
    "        final_dict = {}\n",
    "        final_dict['url'] = output['posts'][index]['thread']['url']\n",
    "        final_dict['site'] = output['posts'][index]['thread']['site_full']\n",
    "        final_dict['site_categories'] = output['posts'][index]['thread']['site_categories']\n",
    "        final_dict['title'] = output['posts'][index]['thread']['title']\n",
    "        final_dict['published'] = output['posts'][index]['thread']['published']\n",
    "        final_dict['participants_count'] = output['posts'][index]['thread']['participants_count']\n",
    "        final_dict['site_type'] = output['posts'][index]['thread']['site_type']\n",
    "        final_dict['country'] = output['posts'][index]['thread']['country']\n",
    "        final_dict['main_image'] = output['posts'][index]['thread']['main_image']\n",
    "        final_dict['domain_rank'] = output['posts'][index]['thread']['domain_rank']\n",
    "        final_dict['facebook_likes'] = output['posts'][index]['thread']['social']['facebook']['likes']\n",
    "        final_dict['facebook_comments'] = output['posts'][index]['thread']['social']['facebook']['comments']\n",
    "        final_dict['facebook_comments'] = output['posts'][index]['thread']['social']['facebook']['shares']\n",
    "        final_dict['author'] = output['posts'][index]['author']\n",
    "        final_dict['title'] = output['posts'][index]['title']\n",
    "        final_dict['text'] = output['posts'][index]['text']\n",
    "        final_dict['language'] = output['posts'][index]['language']\n",
    "        final_dict['rating'] = output['posts'][index]['rating']\n",
    "        final_dict['rating'] = output['posts'][index]['rating']\n",
    "        final_dict['crawled'] = output['posts'][index]['crawled']\n",
    "        final_list.append(final_dict)\n",
    "    output = webhoseio.get_next()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "34680"
      ]
     },
     "execution_count": 260,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Total scraped articles\n",
    "len(final_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final = pd.DataFrame(final_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(34680, 17)"
      ]
     },
     "execution_count": 263,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_final.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Label the articles based on their factual report\n",
    "After scraping articles we need to label them as the JSON object donot return type of factual reporting each article. Therefore, I am labeling each articles based on which site they belong(factual report of each of this site is given in https://mediabiasfactcheck.com.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create lists of websites based on their factual report\n",
    "high_factual_reporting_sites = ['cnbc','abc','japantimes','livescience','rabble','bostonreview','healthline']\n",
    "mixed_factual_reportng_sites = ['ecowatch','buzzfeed','cnn','splinternews','ancient-code','politicaldig.com','thenewcivilrightsmovement']\n",
    "low_factual_reportng_sites = ['trueviralnews','ilovemyfreedom','syrianews','americanthinker','southafricatoday','breaking911','100percentfedup','ageofautism','allnewspipeline']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define a function to lable each article\n",
    "def factual_report (row):\n",
    "    for each_value in high_factual_reporting_sites:\n",
    "        if each_value in row['site']:\n",
    "            return 'High'\n",
    "    for each_value in mixed_factual_reportng_sites:\n",
    "        if each_value in row['site']:\n",
    "            return 'Mixed'\n",
    "    for each_value in low_factual_reportng_sites:\n",
    "        if each_value in row['site']:\n",
    "            return 'Low'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final['factual_reporting'] = df_final.apply (lambda row: factual_report(row), axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Mixed    17159\n",
       "High     13035\n",
       "Low       4486\n",
       "Name: factual_reporting, dtype: int64"
      ]
     },
     "execution_count": 271,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## labled articles in each category\n",
    "df_final['factual_reporting'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [],
   "source": [
    "# moving this information to csv file for further use\n",
    "df_final.to_csv('./data/news_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
